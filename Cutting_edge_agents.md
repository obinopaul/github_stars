  
----------------------------------------

### Open-LLM-VTuber
**URL**: `https://github.com/Open-LLM-VTuber/Open-LLM-VTuber`
**Purpose**: An open-source, offline-capable framework for creating a voice-interactive AI companion featuring a Live2D avatar. It enables real-time, private, and cross-platform conversations with a customizable virtual character. The project's novelty lies in its comprehensive integration of local AI models for a complete, self-hosted virtual being experience.

**Key Components**:
* **Modular AI Backend**: A highly extensible backend that allows for interchangeable modules for LLMs (Ollama, GGUF, vLLM), Automatic Speech Recognition (sherpa-onnx, Faster-Whisper), and Text-to-Speech (MeloTTS, Coqui-TTS). This modularity is a reusable asset for building custom AI pipelines.
* **Live2D Web Frontend**: A client-side interface that renders animated Live2D models. It receives backend commands to control character expressions and actions, providing a dynamic visual component that can be adapted for other virtual agent projects.
* **Visual Perception Module**: A component that leverages camera feeds or screen captures as input for the LLM. This allows the AI to perceive and react to visual data, making it a powerful, reusable tool for creating context-aware applications.

  
----------------------------------------

